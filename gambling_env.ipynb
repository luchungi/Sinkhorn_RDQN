{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "device = 'cpu'\n",
    "\n",
    "from gambling.env import Env\n",
    "from gambling.train import get_robustq_params_dicts, start_writer\n",
    "from agent.q import QFunc\n",
    "from agent.DQN import DQN, GUSRDQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0 # results in paper uses seeds 0,...,99\n",
    "alpha = 1.2\n",
    "beta = 2.0\n",
    "num_samples_given = 5\n",
    "neg_reward_factor = 5.\n",
    "\n",
    "env_batch_size = 16\n",
    "dqn_train_steps = 10000\n",
    "rdqn_train_steps = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncertainty in Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "# Estimate alpha and beta from sample mean and variance\n",
    "true_mean = alpha/(alpha + beta)\n",
    "env = Env(int(num_samples_given), alpha=alpha, beta=beta, seed=seed)\n",
    "obs = env.reset()\n",
    "mean_hat = obs.mean().item()\n",
    "var_hat = obs.var().item()\n",
    "alpha_hat = mean_hat**2*(1 - mean_hat)/var_hat - mean_hat\n",
    "beta_hat = alpha_hat*(1/mean_hat - 1)\n",
    "\n",
    "print(f'alpha: {alpha}, beta: {beta}')\n",
    "print(f'mean = {true_mean:.3f}, var = {alpha*beta/((alpha + beta)**2 * (alpha + beta + 1)):.3f}')\n",
    "print(f'alpha_hat: {alpha_hat:.3f}, beta_hat: {beta_hat:.3f}')\n",
    "print(f'mean_hat = {mean_hat:.3f}, var_hat = {var_hat:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "action_space = torch.tensor([-1,0,1], dtype=torch.float32)\n",
    "agent = DQN(\n",
    "    obs_dim=1,\n",
    "    num_actions=action_space.shape[0],\n",
    "    qfunc=QFunc(1, [64, 64], action_space.shape[0]),\n",
    "    lr=0.001,\n",
    "    batch_size=64,\n",
    "    epsilon=0.1,\n",
    "    discount=0.95,\n",
    "    buffer_max_length=10000,\n",
    "    clone_steps=32,\n",
    "    train_steps=1,\n",
    "    n_batches=1,\n",
    "    n_epochs=1,\n",
    "    clip_gradients=True,\n",
    "    device='cpu',\n",
    "    seed=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'alpha_hat: {alpha_hat}, beta_hat: {beta_hat}')\n",
    "env = Env(env_batch_size, alpha=alpha_hat, beta=beta_hat, neg_reward_factor=neg_reward_factor, seed=seed)\n",
    "reward_list = []\n",
    "obs = env.reset()\n",
    "action = agent.agent_start(obs)\n",
    "obs, reward, _, _ = env.step(action)\n",
    "reward_list.append(reward.mean())\n",
    "for i in tqdm(range(dqn_train_steps)):\n",
    "    action = agent.agent_step(reward, obs)\n",
    "    obs, reward, _, _ = env.step(action)\n",
    "    reward_list.append(reward.mean())\n",
    "agent.agent_end(reward, obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'alpha: {alpha}, beta: {beta}')\n",
    "agent.training_mode = False\n",
    "eval_seed = 12345\n",
    "env = Env(100, alpha, beta, neg_reward_factor=neg_reward_factor, seed=eval_seed)\n",
    "obs = env.reset()\n",
    "reward_list = []\n",
    "for i in tqdm(range(int(1e4))):\n",
    "    action = agent.get_action(obs)\n",
    "    obs, reward, _, _ = env.step(action)\n",
    "    reward_list.append(reward.mean())\n",
    "print(f'Average reward: {np.mean(reward_list):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robust DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "action_space = torch.tensor([-1,0,1])\n",
    "discount = 0.95\n",
    "eps_greedy = 0.1 # epsilon greedy parameter\n",
    "buffer_max_length = int(1e4)\n",
    "clone_steps = 32\n",
    "train_steps = 1\n",
    "agent_batch_size = 64\n",
    "n_batches = 1\n",
    "n_epochs = 1\n",
    "robustq_lr = 1e-3\n",
    "architecture = [64, 64]\n",
    "\n",
    "robustq = QFunc(1, architecture, action_space.shape[0]).to(device)\n",
    "\n",
    "delta = 1e-4 # regularisation parameter for Sinkhorn distance\n",
    "epsilon = 0.1 # Sinkhorn distance\n",
    "norm_ord = 1\n",
    "lamda_init = 0. # initial lambda\n",
    "lamda_max_iter = 100\n",
    "lamda_step_size = 10 # step size for learning rate scheduler\n",
    "lamda_gamma = 10. # gamma for learning rate scheduler\n",
    "lamda_lr = 0.02 # learning rate for lambda\n",
    "n_outer = 1 # not used in this algorithm but used in logging by writer\n",
    "n_inner = 200 # number of samples from nu to calc inner expectations\n",
    "\n",
    "simulator_params, model_params = get_robustq_params_dicts(vars().copy())\n",
    "writer = start_writer(simulator_params, model_params, model_name='GUSRDQN')\n",
    "\n",
    "robustdqn_agent = GUSRDQN(1, action_space.shape[0], discount, action_space, neg_reward_factor, epsilon, delta, n_inner, lamda_init, lamda_lr, lamda_max_iter, lamda_step_size, lamda_gamma, norm_ord, robustq, eps_greedy, buffer_max_length, clone_steps, train_steps, agent_batch_size, n_batches, n_epochs, robustq_lr, device=device, seed=seed, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'alpha_hat: {alpha_hat}, beta_hat: {beta_hat}')\n",
    "env = Env(env_batch_size, alpha=alpha_hat, beta=beta_hat, neg_reward_factor=neg_reward_factor, seed=seed)\n",
    "obs = env.reset()\n",
    "action = action_space[robustdqn_agent.agent_start(obs)]\n",
    "obs, reward, _, _ = env.step(action)\n",
    "for i in tqdm(range(rdqn_train_steps)):\n",
    "    action = action_space[robustdqn_agent.agent_step(reward, obs)]\n",
    "    obs, reward, _, _ = env.step(action)\n",
    "robustdqn_agent.agent_end(reward, obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'alpha: {alpha}, beta: {beta}')\n",
    "robustdqn_agent.training_mode = False\n",
    "env = Env(100, alpha, beta, neg_reward_factor=neg_reward_factor, seed=None)\n",
    "obs = env.reset()\n",
    "reward_list = []\n",
    "for i in tqdm(range(int(1e4))):\n",
    "    action = robustdqn_agent.get_action(obs)\n",
    "    obs, reward, _, _ = env.step(action)\n",
    "    reward_list.append(reward.mean())\n",
    "print(f'Average reward: {np.mean(reward_list):.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robustq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
